{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacob Linder\n",
    "<p>I have a pretty good background in stocks and options, which will make it easier for our group get an idea of what information is important and which may be worth discarding. Additionally, I bring relevant python experience to the table, having worked with options data scraped from APIs in the past, which will allow us to get relevant options data for each post. I also have a lot of personal project based experience with Linux, which will be handy if we want to set this running on a server to continually collect data and make observations.</p>\n",
    "\n",
    "<p> ADD IN: What areas/skills/domains does the team member presently identify with?\n",
    "    Into which areas/skills/domains would the team member like to grow? </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logan Stout\n",
    "<p>I am a hobby investor trading options profitably for the last four years. With my experience, I have a soild grasp on the mechanisms by which securities and their option prices develop as a result of market movements, statistics, and media. As a controls engineer by trade, I have a handle on control systems and managing system outputs based on constantly changing updated data. My strengths include high level project planning and file processing. By approaching this project with the data being mixed with other text in a forum, I hope to gain experience in web scraping and text parsing.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who might be interested?\n",
    "<p>The purpose of our data is to be able to observe if the community at WallStreetBets is actually having any impact on the stocks they suggest, in aggregate. Additionally, we intend to quantify the accuracy of the communities suggestions as a whole by comparing the prices of the underlying equities at the expiry of the contract to the strike of the suggested contract. We will increase the accuracy of our quantification by including the price of the equity at the time of the post. </p>\n",
    "<p>If there is some measure of effect (or even accuracy) to the predicitons on WSB, any trader or alogrithmic trading firm would be eager to incorporate that information into their programs.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is the data limited?\n",
    "<p>The core of the data collection method is scraping text from posts and comments on Reddit in search of stock market positions. This is simple for the ideal case, where a given comment will have a string of text in the format of the stock ticker, date, strike price, and option type (put or call). PSTH 3/19/21 20c, for example, would be a relatively simple string to parse as it provides all of the required information in sequence. Unfortunately, no standardized format is enforced; this means that the same information could be communicated by any permutation containing all four items or even using full sentences with context elsewhere in the post. By not having adequately sophisticated parsing, it is probable that the amount of data collected will be significantly less than what is totally available. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How was the data created?\n",
    "<p>All of the position data came from anonymously submitted posts and comments on Reddit.com/r/wallstreetbets. Individual stock data is generated by investors making trades on the stock market, most commonly the New York Stock Exchange and the Nasdaq Stock Market. The specific data used is taken from Yahoo Finance quotes, which receives it from a company called ICE Data Service, who takes it from the exchanges themselves.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What access rights exist on your data? How will you make your data available?\n",
    "<p> Currently, there are no obvious restrictions on data usage or collection. Any internet connection can access both Reddit and Yahoo Finance without even having a registered account. However, Reddit may have some control over the content posted on their website, and Yahoo may restrict their data from being used for commercial purposes (figure we should check).</p>\n",
    "<p> We will make the data available originally as a simple bulk JSON file of all the suggestions make on WSB, as well as the post text connected to that suggestion. Additionally, we intend to compute an aggregated \"sentiment\" of WSB, as measured by number and direction of suggestions, combined with the number of upvotes and comments on the posts themselves. Further down the line, we would like to present the data by ticker, rating current WSB sentiment as well as the historic amount of sway WSB has had over a particular stock, if available. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_time = datetime.datetime(2020,1,15,0,0).timestamp()\n",
    "after_time = int(after_time)\n",
    "before_time = datetime.datetime(2020,1,16,0,0).timestamp()\n",
    "before_time = int(before_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts = requests.get('https://api.pushshift.io/reddit/submission/search/?after={}&sort_type=created_utc&sort=asc&subreddit=wallstreetbets&size=150'.format(after_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_posts.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_count = 0\n",
    "flair_dict = {'None':0}\n",
    "for post_dict in data['data']:\n",
    "    try:\n",
    "        flair = post_dict['link_flair_text']\n",
    "    except KeyError:\n",
    "        flair = \"None\"\n",
    "        flair_dict[flair] += 1\n",
    "    if flair != \"None\":\n",
    "        try:\n",
    "            flair_dict[flair] += 1\n",
    "        except KeyError:\n",
    "            flair_dict[post_dict['link_flair_text']] = 1\n",
    "\n",
    "sum(flair_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'None': 26,\n",
       " 'Shitpost': 18,\n",
       " 'Technicals': 1,\n",
       " 'Options': 5,\n",
       " 'Fundamentals': 3,\n",
       " 'Satire': 4,\n",
       " 'YOLO': 3,\n",
       " 'Meme': 3,\n",
       " 'Discussion': 16,\n",
       " 'DD': 7,\n",
       " 'Gain': 5,\n",
       " 'Daily Discussion': 1,\n",
       " 'Storytime': 1,\n",
       " 'Stocks': 4,\n",
       " 'Loss': 3}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_flairs = ['DD','Options','Stocks','Fundamentals','Technicals','YOLO','Discussion']\n",
    "good_posts = []\n",
    "max_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for post_dict in data['data']:\n",
    "    try:\n",
    "        post_dict['link_flair_text']\n",
    "    except KeyError:\n",
    "        continue\n",
    "    if post_dict['link_flair_text'] in good_flairs:\n",
    "        good_posts.append(post_dict)\n",
    "        if post_dict['created_utc'] > max_time:\n",
    "            max_time = post_dict['created_utc']\n",
    "len(good_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37499"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_day = 86400\n",
    "max_time - after_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_time = datetime.datetime(2020,1,15,0,0).timestamp()\n",
    "after_time = int(after_time)\n",
    "stop_time = int(datetime.datetime(2020,1,20,0,0).timestamp())\n",
    "\n",
    "all_posts = requests.get('https://api.pushshift.io/reddit/submission/search/?after={}&sort_type=created_utc&sort=asc&subreddit=wallstreetbets&size=150'.format(after_time))\n",
    "data = all_posts.json()\n",
    "\n",
    "good_flairs = ['DD','Options','Stocks','Fundamentals','Technicals','YOLO','Discussion']\n",
    "good_posts = []\n",
    "max_time = 0\n",
    "all_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "while (after_time <= stop_time):\n",
    "    for post_dict in data['data']:\n",
    "        try:\n",
    "            post_dict['link_flair_text']\n",
    "        except KeyError:\n",
    "            continue\n",
    "        if post_dict['link_flair_text'] in good_flairs:\n",
    "            good_posts.append(post_dict)\n",
    "            if post_dict['created_utc'] > max_time:\n",
    "                max_time = post_dict['created_utc']\n",
    "    #recalculate new time to get next 100 posts\n",
    "    after_time = after_time + (max_time - after_time)\n",
    "    time.sleep(2)\n",
    "    all_posts = requests.get('https://api.pushshift.io/reddit/submission/search/?after={}&sort_type=created_utc&sort=asc&subreddit=wallstreetbets&size=150'.format(after_time))\n",
    "    data = all_posts.json()\n",
    "    \n",
    "    # write to file\n",
    "    # record max time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "641"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
